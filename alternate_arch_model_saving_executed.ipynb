{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d703b6c6",
   "metadata": {
    "id": "20565b66"
   },
   "source": [
    "![Untitled%20presentation.png](attachment:Untitled%20presentation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab718059",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKO9SBEOWJHK",
    "outputId": "fa441de8-0526-4203-ac62-f568bd8ab55c"
   },
   "outputs": [],
   "source": [
    "#! tar -xvf ../images.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42237de6",
   "metadata": {
    "id": "546a571a"
   },
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# my_tar = tarfile.open('../dataset/images.tar')\n",
    "# my_tar.extractall('../dataset/images/') # specify which folder to extract to\n",
    "# my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44f03c79",
   "metadata": {
    "id": "08976da8"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "233e8155",
   "metadata": {
    "id": "b92af9c1"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86520427",
   "metadata": {
    "id": "fccf7a49"
   },
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa0443f",
   "metadata": {
    "id": "ef3becd7"
   },
   "outputs": [],
   "source": [
    "# data_dir = '../dataset/images/images/'\n",
    "data_dir = \"./dataset/images/\"\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c00b0ee",
   "metadata": {
    "id": "9f606f92"
   },
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "image_files = os.listdir(data_dir)\n",
    "# image_datasets = datasets.ImageFolder(data_dir, data_transforms)\n",
    "# dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17aea6e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4949bd5c",
    "outputId": "f6740c30-35b1-484e-ce78-0727fa4aaaeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n"
     ]
    }
   ],
   "source": [
    "image_tensor = Image.open(data_dir + image_files[0])\n",
    "image_tensor = image_tensor.convert('RGB')\n",
    "image_tensor = data_transforms(image_tensor)\n",
    "image_tensor = torch.unsqueeze(image_tensor, 0)\n",
    "error_files = []\n",
    "count = 0\n",
    "for image_file in image_files[1:]:\n",
    "    try:\n",
    "        img = Image.open(data_dir + image_file)\n",
    "        img = img.convert('RGB')\n",
    "        img_t = data_transforms(img)\n",
    "        img_t = torch.unsqueeze(img_t, 0)\n",
    "        image_tensor = torch.cat((image_tensor, img_t), 0)\n",
    "    except:\n",
    "        error_files.append(image_file)\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292739ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a099a4ec",
    "outputId": "63017be6-86c3-40f3-db3b-0c55cec4f5d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef001e8",
   "metadata": {
    "id": "xt7iHj6RWuZq"
   },
   "outputs": [],
   "source": [
    "# torch.save(image_tensor, \"image_tensor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4b4df3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4876e9da",
    "outputId": "3266d7da-f9cf-4ba4-8405-782f55c87762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 100])\n"
     ]
    }
   ],
   "source": [
    "X1_file =\"./dataset/word2vec_emb_tensor.pkl\"\n",
    "# X1_file = \"./word2vec_emb_tensor.pkl\"\n",
    "X1 = torch.load(X1_file)\n",
    "print(X1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3a82909",
   "metadata": {
    "id": "ssUAF2urXZN1"
   },
   "outputs": [],
   "source": [
    "X1_t = X1.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a7bc222",
   "metadata": {
    "id": "4ef43172"
   },
   "outputs": [],
   "source": [
    "# Convolution Encoder\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        #Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.lin1 = nn.Linear(12544, 512)\n",
    "        self.lin2 = nn.Linear(512, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "#         print(x.size())\n",
    "        x = self.pool(x)\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "#         print(x.size())\n",
    "        x = nn.ReLU()(self.lin1(x))\n",
    "        x = nn.ReLU()(self.lin2(x))\n",
    "#         print(x.size())\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bd9ef72",
   "metadata": {
    "id": "0895a3df"
   },
   "outputs": [],
   "source": [
    "# Encoder for real numbered matrix\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.enc_linear1 = nn.Linear(input_dim, input_dim//2)\n",
    "        self.enc_linear2 = nn.Linear(input_dim//2, embedding_dim)\n",
    "        self.emb = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.enc_linear2(x)\n",
    "        self.emb = x # return embedding from encoder\n",
    "        return x # use x for training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ffdd31b",
   "metadata": {
    "id": "67daef28"
   },
   "outputs": [],
   "source": [
    "# Convolution Decoder\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        #Decoder\n",
    "        self.lin1 = nn.Linear(embedding_dim, 512)\n",
    "        self.lin2 = nn.Linear(512, 12544)\n",
    "        self.deconv1 = nn.ConvTranspose2d(1, 16, 3, stride=2, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(16, 3, 3, stride=1, padding = 2, output_padding=0)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.lin1(x))\n",
    "        x = nn.ReLU()(self.lin2(x)).view(-1, 1, 112, 112)\n",
    "        # print(x.size())\n",
    "        x = nn.ReLU()(self.deconv1(x))\n",
    "        x = self.deconv2(x)\n",
    "        # print(x.size())\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90bfa697",
   "metadata": {
    "id": "35a88a56"
   },
   "outputs": [],
   "source": [
    "class Arch(nn.Module):\n",
    "    def __init__(self, embedding_dim, book_size, word2vec_size):\n",
    "        super().__init__()\n",
    "        self.conv_encoder = ConvEncoder(embedding_dim)\n",
    "        self.encoder_row = Encoder(word2vec_size, embedding_dim)\n",
    "        self.encoder_col = Encoder(book_size, embedding_dim)\n",
    "        self.conv_decoder = ConvDecoder(embedding_dim)\n",
    "        self.intermediate_lin1 = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        \n",
    "    def forward(self, matrices): # replace with one cell recon\n",
    "        # currently 3 inputs - [image batch, word vec row batch, word vec col batch]\n",
    "        # ensure the entity of interest is the row entity in m eg: books x images => books are entity of interest\n",
    "        m_conv_encoded = self.conv_encoder(matrices[0])\n",
    "        m_row_encoded = self.encoder_row(matrices[1])\n",
    "        m_col_encoded = self.encoder_col(matrices[2])\n",
    "        m_row_cat = torch.cat((m_conv_encoded, m_row_encoded), axis = 1)\n",
    "        m_row_emb = self.intermediate_lin1(m_row_cat)\n",
    "        X0_prime = self.conv_decoder(m_row_emb)\n",
    "        \n",
    "        X1_prime = torch.mm(m_row_emb, m_col_encoded.transpose(0, 1))\n",
    "        return X0_prime, X1_prime, m_row_emb\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05ab0c48",
   "metadata": {
    "id": "b66d86ab"
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "epoch_count = 50\n",
    "convergence_threshold = 1e-3\n",
    "batch_size = 50\n",
    "book_size = 5000\n",
    "word2vec_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e2a8643",
   "metadata": {
    "id": "a3183ad8"
   },
   "outputs": [],
   "source": [
    "net = Arch(50, book_size, word2vec_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab88005a",
   "metadata": {
    "id": "69njbG70mUxY"
   },
   "outputs": [],
   "source": [
    "def test(sample):\n",
    "    m_conv_encoded = net.conv_encoder(sample[0].unsqueeze(0))\n",
    "    m_row_encoded = net.encoder_row(sample[1].unsqueeze(0))\n",
    "    m_row_cat = torch.cat((m_conv_encoded, m_row_encoded), axis = 1)\n",
    "    m_row_emb = net.intermediate_lin1(m_row_cat)\n",
    "    return m_row_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d0012b7",
   "metadata": {
    "id": "ymj1WaoZmu7A"
   },
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    # embeddings = []\n",
    "    embeddings = np.empty((book_size, embedding_dim), float)\n",
    "    for i in range(0, book_size):\n",
    "      sample = []\n",
    "      sample.append(image_tensor[i])\n",
    "      sample.append(X1[i])\n",
    "      # embeddings.append(test(sample).numpy())\n",
    "      embeddings[i] = test(sample).detach().numpy()\n",
    "      # print(embeddings.shape)\n",
    "    np.save(\"embeddings_new_arch.npy\", embeddings)\n",
    "    # return torch.Tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba720d05",
   "metadata": {
    "id": "66d4cf10"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # training\n",
    "    prev_losses = []\n",
    "    for epoch in range(0,epoch_count):\n",
    "        avg_loss = 0\n",
    "        counter = 0\n",
    "        for r_count in range(0, 5000, batch_size):\n",
    "            matrix0 = (image_tensor[r_count:r_count + batch_size])\n",
    "            matrix1 = (X1[r_count:r_count + batch_size])\n",
    "            for c_count in range(0, 100, batch_size):\n",
    "              counter += 1\n",
    "              matrix2 = (X1_t[c_count:c_count + batch_size])\n",
    "              m0_prime, m1_prime, m_row_emb = net.forward([matrix0, matrix1, matrix2])\n",
    "              loss = criterion(matrix0, m0_prime) + criterion(matrix1[:,c_count: c_count+batch_size], m1_prime)\n",
    "              avg_loss += loss.item()\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "        per_epoch_loss = avg_loss/counter\n",
    "        prev_losses.append(per_epoch_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Average loss for epoch {epoch} = {per_epoch_loss}\")\n",
    "        if  (epoch > 100) and (len(prev_losses) > 0) and (abs(prev_losses[-1] - loss) < convergence_threshold):\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        prev_losses.append(loss)\n",
    "        torch.save(net.state_dict(), f\"./alternate_arch_epoch{epoch}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1bfde8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtIJx6q4yK10",
    "outputId": "57f73fe7-a216-481b-f979-ad80e81750ba"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b159b7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "04ec4646",
    "outputId": "486c58a1-43eb-48e9-bedc-e1315f16c720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 0 = 1.8685082274675369\n",
      "Average loss for epoch 10 = 1.8664999401569367\n",
      "Average loss for epoch 20 = 1.8654325634241105\n",
      "Average loss for epoch 30 = 1.864666109085083\n",
      "Average loss for epoch 40 = 1.8645415729284287\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d34a0f02",
   "metadata": {
    "id": "XSXOWNB5xl71"
   },
   "outputs": [],
   "source": [
    "embedding_dim=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59420037",
   "metadata": {
    "id": "f7b1387e"
   },
   "outputs": [],
   "source": [
    "get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0775118",
   "metadata": {
    "id": "yZ4j_luXrH_e"
   },
   "outputs": [],
   "source": [
    "# torch.save(emb, \"embeddings_new_arch.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68063a5",
   "metadata": {
    "id": "6B85icf4sz8R"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('embeddings_new_arch.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9db142",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpsIc3fsyP7U",
    "outputId": "9ad012e5-e0e8-49d4-f0c1-661fba17b32a"
   },
   "outputs": [],
   "source": [
    "np.load(\"embeddings_new_arch.npy\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1ca67",
   "metadata": {
    "id": "L4idnc8tymQX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "alternate_arch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
