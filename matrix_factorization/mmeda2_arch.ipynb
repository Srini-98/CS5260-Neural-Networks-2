{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b664a4",
   "metadata": {
    "id": "20565b66"
   },
   "source": [
    "![mmeda2.jpg](mmeda2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cabc4960",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKO9SBEOWJHK",
    "outputId": "fa441de8-0526-4203-ac62-f568bd8ab55c"
   },
   "outputs": [],
   "source": [
    "#! tar -xvf ../images.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a311357",
   "metadata": {
    "id": "546a571a"
   },
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# my_tar = tarfile.open('../dataset/images.tar')\n",
    "# my_tar.extractall('../dataset/images/') # specify which folder to extract to\n",
    "# my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36931050",
   "metadata": {
    "id": "08976da8"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54eb3441",
   "metadata": {
    "id": "b92af9c1"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2dca66",
   "metadata": {
    "id": "fccf7a49"
   },
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8ec62a8",
   "metadata": {
    "id": "ef3becd7"
   },
   "outputs": [],
   "source": [
    "# data_dir = '../dataset/images/images/'\n",
    "data_dir = \"../dataset/images/\"\n",
    "input_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44ca23ef",
   "metadata": {
    "id": "9f606f92"
   },
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "image_files = os.listdir(data_dir)\n",
    "# image_datasets = datasets.ImageFolder(data_dir, data_transforms)\n",
    "# dataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb15ab62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4949bd5c",
    "outputId": "f6740c30-35b1-484e-ce78-0727fa4aaaeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n"
     ]
    }
   ],
   "source": [
    "image_tensor = Image.open(data_dir + image_files[0])\n",
    "image_tensor = image_tensor.convert('RGB')\n",
    "image_tensor = data_transforms(image_tensor)\n",
    "image_tensor = torch.unsqueeze(image_tensor, 0)\n",
    "error_files = []\n",
    "count = 0\n",
    "for image_file in image_files[1:]:\n",
    "    try:\n",
    "        img = Image.open(data_dir + image_file)\n",
    "        img = img.convert('RGB')\n",
    "        img_t = data_transforms(img)\n",
    "        img_t = torch.unsqueeze(img_t, 0)\n",
    "        image_tensor = torch.cat((image_tensor, img_t), 0)\n",
    "    except:\n",
    "        error_files.append(image_file)\n",
    "    count += 1\n",
    "    if count % 100 == 0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831b6100",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a099a4ec",
    "outputId": "63017be6-86c3-40f3-db3b-0c55cec4f5d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cd3a800",
   "metadata": {
    "id": "xt7iHj6RWuZq"
   },
   "outputs": [],
   "source": [
    "# torch.save(image_tensor, \"image_tensor.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a43de0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4876e9da",
    "outputId": "3266d7da-f9cf-4ba4-8405-782f55c87762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 100])\n"
     ]
    }
   ],
   "source": [
    "X1_file =\"../dataset/word2vec_emb_tensor.pkl\"\n",
    "# X1_file = \"./word2vec_emb_tensor.pkl\"\n",
    "X1 = torch.load(X1_file)\n",
    "print(X1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b77e120",
   "metadata": {
    "id": "ssUAF2urXZN1"
   },
   "outputs": [],
   "source": [
    "X1_t = X1.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48391999",
   "metadata": {
    "id": "4ef43172"
   },
   "outputs": [],
   "source": [
    "# Convolution Encoder\n",
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        #Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)  \n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.lin1 = nn.Linear(12544, 512)\n",
    "        self.lin2 = nn.Linear(512, embedding_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "#         print(x.size())\n",
    "        x = self.pool(x)\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "#         print(x.size())\n",
    "        x = nn.ReLU()(self.lin1(x))\n",
    "        x = nn.ReLU()(self.lin2(x))\n",
    "#         print(x.size())\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3fb19e2",
   "metadata": {
    "id": "0895a3df"
   },
   "outputs": [],
   "source": [
    "# Encoder for real numbered matrix\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.enc_linear1 = nn.Linear(input_dim, input_dim//2)\n",
    "        self.enc_linear2 = nn.Linear(input_dim//2, embedding_dim)\n",
    "        self.emb = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.enc_linear2(x)\n",
    "        self.emb = x # return embedding from encoder\n",
    "        return x # use x for training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b88e82a",
   "metadata": {
    "id": "67daef28"
   },
   "outputs": [],
   "source": [
    "# Convolution Decoder\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        #Decoder\n",
    "        self.lin1 = nn.Linear(embedding_dim, 512)\n",
    "        self.lin2 = nn.Linear(512, 12544)\n",
    "        self.deconv1 = nn.ConvTranspose2d(1, 16, 3, stride=2, output_padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(16, 3, 3, stride=1, padding = 2, output_padding=0)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.lin1(x))\n",
    "        x = nn.ReLU()(self.lin2(x)).view(-1, 1, 112, 112)\n",
    "        # print(x.size())\n",
    "        x = nn.ReLU()(self.deconv1(x))\n",
    "        x = self.deconv2(x)\n",
    "        # print(x.size())\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09fe1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder for real numbered matrix\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.enc_linear1 = nn.Linear(embedding_dim, input_dim // 2)\n",
    "        self.enc_linear2 = nn.Linear(input_dim//2, input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.enc_linear2(x)\n",
    "        return x # use x for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "436e4f45",
   "metadata": {
    "id": "35a88a56"
   },
   "outputs": [],
   "source": [
    "class Arch(nn.Module):\n",
    "    def __init__(self, embedding_dim, book_size, word2vec_size):\n",
    "        super().__init__()\n",
    "        self.conv_encoder = ConvEncoder(embedding_dim)\n",
    "        self.encoder_row = Encoder(word2vec_size, embedding_dim)\n",
    "        self.encoder_col = Encoder(book_size, embedding_dim)\n",
    "        self.conv_decoder = ConvDecoder(embedding_dim)\n",
    "        self.decoder_row = Decoder(word2vec_size, embedding_dim)\n",
    "        self.decoder_col = Decoder(book_size, embedding_dim)\n",
    "        self.intermediate_lin1 = nn.Linear(embedding_dim * 2, embedding_dim)\n",
    "        \n",
    "    def forward(self, matrices): # replace with one cell recon\n",
    "        # currently 3 inputs - [image batch, word vec row batch, word vec col batch]\n",
    "        # ensure the entity of interest is the row entity in m eg: books x images => books are entity of interest\n",
    "        m_conv_encoded = self.conv_encoder(matrices[0])\n",
    "        m_row_encoded = self.encoder_row(matrices[1])\n",
    "        m_col_encoded = self.encoder_col(matrices[2])\n",
    "        m_row_cat = torch.cat((m_conv_encoded, m_row_encoded), axis = 1)\n",
    "        m_row_emb = self.intermediate_lin1(m_row_cat)\n",
    "        X0_prime = self.conv_decoder(m_row_emb)\n",
    "        X1_row_prime = self.decoder_row(m_row_encoded)\n",
    "        X1_col_prime = self.decoder_col(m_col_encoded)\n",
    "        \n",
    "        X1_prime = torch.mm(m_row_emb, m_col_encoded.transpose(0, 1))\n",
    "        return X0_prime, X1_prime, m_row_emb, X1_row_prime, X1_col_prime\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bf47d87",
   "metadata": {
    "id": "b66d86ab"
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "epoch_count = 50\n",
    "convergence_threshold = 1e-3\n",
    "batch_size = 50\n",
    "book_size = 5000\n",
    "word2vec_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5f0d1a5",
   "metadata": {
    "id": "a3183ad8"
   },
   "outputs": [],
   "source": [
    "net = Arch(50, book_size, word2vec_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ffab8e8d",
   "metadata": {
    "id": "69njbG70mUxY"
   },
   "outputs": [],
   "source": [
    "def test(sample):\n",
    "    m_conv_encoded = net.conv_encoder(sample[0].unsqueeze(0))\n",
    "    m_row_encoded = net.encoder_row(sample[1].unsqueeze(0))\n",
    "    m_row_cat = torch.cat((m_conv_encoded, m_row_encoded), axis = 1)\n",
    "    m_row_emb = net.intermediate_lin1(m_row_cat)\n",
    "    return m_row_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48949721",
   "metadata": {
    "id": "ymj1WaoZmu7A"
   },
   "outputs": [],
   "source": [
    "def get_embeddings():\n",
    "    # embeddings = []\n",
    "    embeddings = np.empty((book_size, embedding_dim), float)\n",
    "    for i in range(0, book_size):\n",
    "      sample = []\n",
    "      sample.append(image_tensor[i])\n",
    "      sample.append(X1[i])\n",
    "      # embeddings.append(test(sample).numpy())\n",
    "      embeddings[i] = test(sample).detach().numpy()\n",
    "      # print(embeddings.shape)\n",
    "    np.save(\"embeddings_alt_arch_with_decoder.npy\", embeddings)\n",
    "    # return torch.Tensor(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9b378b9",
   "metadata": {
    "id": "66d4cf10"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # training\n",
    "    prev_losses = []\n",
    "    for epoch in range(0,epoch_count):\n",
    "        avg_loss = 0\n",
    "        counter = 0\n",
    "        for r_count in range(0, 5000, batch_size):\n",
    "            matrix0 = (image_tensor[r_count:r_count + batch_size])\n",
    "            matrix1 = (X1[r_count:r_count + batch_size])\n",
    "            for c_count in range(0, 100, batch_size):\n",
    "              counter += 1\n",
    "              matrix2 = (X1_t[c_count:c_count + batch_size])\n",
    "              m0_prime, m1_prime, m_row_emb, X1_row_prime, X1_col_prime = net.forward([matrix0, matrix1, matrix2])\n",
    "              loss = criterion(matrix0, m0_prime) + criterion(matrix1[:,c_count: c_count+batch_size], m1_prime) + criterion(matrix1, X1_row_prime) + criterion(matrix2, X1_col_prime)\n",
    "              avg_loss += loss.item()\n",
    "              optimizer.zero_grad()\n",
    "              loss.backward()\n",
    "              optimizer.step()\n",
    "        per_epoch_loss = avg_loss/counter\n",
    "        prev_losses.append(per_epoch_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Average loss for epoch {epoch} = {per_epoch_loss}\")\n",
    "        if  (epoch > 100) and (len(prev_losses) > 0) and (abs(prev_losses[-1] - loss) < convergence_threshold):\n",
    "            print('Convergence!')\n",
    "            break\n",
    "        prev_losses.append(loss)\n",
    "        #torch.save(net.state_dict(), f\"./model/alternate_arch_with_decoder_epoch{epoch}.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc72aacf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AtIJx6q4yK10",
    "outputId": "57f73fe7-a216-481b-f979-ad80e81750ba"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5d51533",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "id": "04ec4646",
    "outputId": "486c58a1-43eb-48e9-bedc-e1315f16c720"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AISWARYA\\Miniconda3\\envs\\seat\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss for epoch 0 = 2.216674911379814\n",
      "Average loss for epoch 10 = 1.9791384303569795\n",
      "Average loss for epoch 20 = 1.8730588269233703\n",
      "Average loss for epoch 30 = 1.8653783345222472\n",
      "Average loss for epoch 40 = 1.8646367859840394\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfb1d856",
   "metadata": {
    "id": "XSXOWNB5xl71"
   },
   "outputs": [],
   "source": [
    "embedding_dim=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "173d823e",
   "metadata": {
    "id": "f7b1387e"
   },
   "outputs": [],
   "source": [
    "get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c978e9a",
   "metadata": {
    "id": "yZ4j_luXrH_e"
   },
   "outputs": [],
   "source": [
    "# torch.save(emb, \"embeddings_new_arch_2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b0b9d5e",
   "metadata": {
    "id": "6B85icf4sz8R"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download('embeddings_new_arch_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb663e",
   "metadata": {
    "id": "L4idnc8tymQX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "alternate_arch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
