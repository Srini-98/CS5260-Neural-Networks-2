{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49717fad",
   "metadata": {},
   "source": [
    "Matrix structure:\n",
    "```\n",
    "++++   ++++   ++++   ++++  ++++\n",
    "+X0+   +X1+   +X2+   +X3+  +X4+\n",
    "++++   ++++   ++++   ++++  ++++\n",
    "\n",
    "X0 - books x word emb\n",
    "X1 - books x googleNet cnn emb\n",
    "X2 - books x inceptionNet cnn emb\n",
    "X3 - books x resnet cnn emb\n",
    "X4 - books x VGG cnn emb\n",
    "\n",
    "E0 - books\n",
    "E1 - word emb\n",
    "E2 - googleNet cnn emb\n",
    "E3 - inceptionNet cnn emb\n",
    "E4 - resnet cnn emb\n",
    "E5 - VGG cnn emb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ab6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60235fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../dataset/'\n",
    "X0_file = dataset_folder + \"word2vec_emb_tensor.pkl\"\n",
    "X1_file = dataset_folder + \"googleNet_cnn_emb_tensor.pkl\"\n",
    "X2_file = dataset_folder + \"inceptionNet_cnn_emb_tensor.pkl\"\n",
    "X3_file = dataset_folder + \"cnn_resnet_emb_tensor.pkl\"\n",
    "X4_file = dataset_folder + \"vgg_cnn_emb_tensor.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b8f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 100])\n",
      "torch.Size([5000, 1000])\n",
      "torch.Size([5000, 1000])\n",
      "torch.Size([5000, 1000])\n",
      "torch.Size([5000, 1000])\n"
     ]
    }
   ],
   "source": [
    "X0 = torch.load(X0_file)\n",
    "print(X0.size())\n",
    "X1 = torch.load(X1_file)\n",
    "print(X1.size())\n",
    "X2 = torch.load(X2_file)\n",
    "print(X2.size())\n",
    "X3 = torch.load(X3_file)\n",
    "print(X3.size())\n",
    "X4 = torch.load(X4_file)\n",
    "print(X4.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3278b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.enc_linear1 = nn.Linear(input_dim, 128)\n",
    "        self.enc_linear2 = nn.Linear(128, embedding_dim)\n",
    "        self.dec_linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.dec_linear2 = nn.Linear(128, input_dim)\n",
    "        self.emb = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.enc_linear2(x)\n",
    "        self.emb = x # return embedding from encoder\n",
    "        x = torch.relu(x)\n",
    "        x = self.dec_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dec_linear2(x)\n",
    "        return x # use x for training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6b38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrix_factorization():\n",
    "    def __init__(self, matrices, entity_list, matrix_entity_mapping, emb_dim):\n",
    "        self.matrices = matrices\n",
    "        self.entity_list = entity_list\n",
    "        self.matrix_entity_mapping = matrix_entity_mapping # {\"E0\": [\"X0\", \"X1\"], \"E1\": [\"X0\"], \"E2\":[\"X1\"]}\n",
    "        self.emb_dim = emb_dim\n",
    "        self.autoencoders = {} # {\"E0\": E0_autoencoder, \"E1\": E1_ae, ...}\n",
    "        self.reconstructed_matrices = {} # {\"X0\": recon_X0, \"X1\": recon_X1, ...}\n",
    "        self.embeddings = {} # {\"E0\": E0_emb, \"E1\": E1_emb, ...}\n",
    "        self.concatenated_matrices = []\n",
    "        self.optim = None\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.batch_size = 50\n",
    "        self.convergence_threshold = 1e-4\n",
    "        self.learning_rate = 0.00001\n",
    "        self.epoch_count = 500\n",
    "        \n",
    "    def init_autoencoders(self):\n",
    "        # initialize autoencoder - one for each entity\n",
    "        for entity, matrices in matrix_entity_mapping.items():\n",
    "            if entity == \"E0\":\n",
    "                C_E0 = matrices[0]\n",
    "                for i in range(1, len(matrices)):\n",
    "                    C_E0 = torch.cat((C_E0, matrices[i]), dim = 1)\n",
    "                print(C_E0.size())\n",
    "                E0_aec = Autoencoder(C_E0.size(1), self.emb_dim)\n",
    "            elif entity == \"E1\":\n",
    "                C_E1 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E1.size())\n",
    "                E1_aec = Autoencoder(C_E1.size(1), self.emb_dim)\n",
    "            elif entity == \"E2\":\n",
    "                C_E2 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E2.size())\n",
    "                E2_aec = Autoencoder(C_E2.size(1), self.emb_dim)\n",
    "            elif entity == \"E3\":\n",
    "                C_E3 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E3.size())\n",
    "                E3_aec = Autoencoder(C_E3.size(1), self.emb_dim)\n",
    "            elif entity == \"E4\":\n",
    "                C_E4 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E4.size())\n",
    "                E4_aec = Autoencoder(C_E4.size(1), self.emb_dim)\n",
    "            elif entity == \"E5\":\n",
    "                C_E5 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E5.size())\n",
    "                E5_aec = Autoencoder(C_E5.size(1), self.emb_dim)\n",
    "                \n",
    "        self.concatenated_matrices = {\"E0\": C_E0, \"E1\": C_E1, \"E2\": C_E2, \"E3\": C_E3, \"E4\": C_E4, \"E5\": C_E5}\n",
    "        self.autoencoders = {\"E0\": E0_aec, \"E1\": E1_aec, \"E2\": E2_aec, \"E3\": E3_aec, \"E4\": E4_aec, \"E5\": E5_aec}\n",
    "        self.optim = torch.optim.SGD(list(E0_aec.parameters()) + list(E1_aec.parameters()) + \\\n",
    "                                     list(E2_aec.parameters())+ list(E3_aec.parameters()) + \\\n",
    "                                     list(E4_aec.parameters()) +list(E5_aec.parameters()), lr = self.learning_rate)\n",
    "    \n",
    "    def train_autoencoder(self):\n",
    "        # training\n",
    "        prev_losses = []\n",
    "        for epoch in range(0,self.epoch_count):\n",
    "            shuffled_indices = {}\n",
    "            avg_loss = {}\n",
    "            ent_emb = {}\n",
    "            for e in self.autoencoders.keys():\n",
    "                shuffled_indices[e] = torch.randperm(self.concatenated_matrices[e].size(0))\n",
    "                ent_emb[e] = torch.zeros(self.concatenated_matrices[e].size(0), self.emb_dim)\n",
    "            \n",
    "            for e in self.concatenated_matrices.keys():\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "                for count in range(0, self.concatenated_matrices[e].size(0), self.batch_size):\n",
    "                    indices = shuffled_indices[e][count:count+self.batch_size] \n",
    "                    minibatch = self.concatenated_matrices[e][indices]\n",
    "                    output = self.autoencoders[e](minibatch)\n",
    "                    ent_emb[e][indices] = self.autoencoders[e].emb # assign emb of the mini batch to entity\n",
    "#                     print(ent_emb[e][indices[0]])\n",
    "                    loss = self.criterion(minibatch, output)\n",
    "                    num_batches += 1\n",
    "                    total_loss += loss\n",
    "                avg_loss[e] = total_loss/num_batches\n",
    "    \n",
    "#             print(ent_emb['E0'][0])\n",
    "            aec_loss = 0\n",
    "    \n",
    "            for v in avg_loss.values():\n",
    "                aec_loss += v\n",
    "#             print(f\"Aec {aec_loss}\")\n",
    "            self.reconstructed_matrices['X0'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E1'], 0, 1))\n",
    "            self.reconstructed_matrices['X1'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E2'], 0, 1))\n",
    "            self.reconstructed_matrices['X2'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E3'], 0, 1))\n",
    "            self.reconstructed_matrices['X3'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E4'], 0, 1))\n",
    "            self.reconstructed_matrices['X4'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E5'], 0, 1))\n",
    "            recon_loss = self.criterion(self.reconstructed_matrices['X0'], self.matrix_entity_mapping[\"E1\"]) + \\\n",
    "                        self.criterion(self.reconstructed_matrices['X1'], self.matrix_entity_mapping[\"E2\"]) + \\\n",
    "                        self.criterion(self.reconstructed_matrices['X2'], self.matrix_entity_mapping[\"E3\"]) + \\\n",
    "                        self.criterion(self.reconstructed_matrices['X3'], self.matrix_entity_mapping[\"E4\"]) + \\\n",
    "                        self.criterion(self.reconstructed_matrices['X4'], self.matrix_entity_mapping[\"E5\"])\n",
    "#             print(f\"recon loss {recon_loss}\")\n",
    "            aec_loss += recon_loss\n",
    "#             print(f\"Total {aec_loss}\")\n",
    "            self.optim.zero_grad()\n",
    "            aec_loss.requires_grad_(True)\n",
    "            aec_loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Average loss for epoch {epoch} = {aec_loss}\")\n",
    "            if  (epoch > 100) and (len(prev_losses) > 0) and (prev_losses[-1] - aec_loss < self.convergence_threshold):\n",
    "                print('Convergence!')\n",
    "                break\n",
    "            prev_losses.append(aec_loss)\n",
    "        \n",
    "    def get_embeddings(self):\n",
    "        for e in self.matrix_entity_mapping.keys():\n",
    "            out = self.autoencoders[e](self.concatenated_matrices[e])\n",
    "            self.embeddings[e] = self.autoencoders[e].emb\n",
    "        return self.embeddings\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94449844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 4100])\n",
      "torch.Size([100, 5000])\n",
      "torch.Size([1000, 5000])\n",
      "torch.Size([1000, 5000])\n",
      "torch.Size([1000, 5000])\n",
      "torch.Size([1000, 5000])\n",
      "Average loss for epoch 0 = 25.892372131347656\n",
      "Average loss for epoch 10 = 24.21759796142578\n",
      "Average loss for epoch 20 = 23.493083953857422\n",
      "Average loss for epoch 30 = 23.044540405273438\n",
      "Average loss for epoch 40 = 22.731101989746094\n",
      "Average loss for epoch 50 = 22.499862670898438\n",
      "Average loss for epoch 60 = 22.32384490966797\n",
      "Average loss for epoch 70 = 22.183021545410156\n",
      "Average loss for epoch 80 = 22.06642723083496\n",
      "Average loss for epoch 90 = 21.967226028442383\n",
      "Average loss for epoch 100 = 21.881023406982422\n",
      "Average loss for epoch 110 = 21.80487823486328\n",
      "Average loss for epoch 120 = 21.73670196533203\n",
      "Average loss for epoch 130 = 21.67487335205078\n",
      "Average loss for epoch 140 = 21.618167877197266\n",
      "Average loss for epoch 150 = 21.56584930419922\n",
      "Average loss for epoch 160 = 21.517314910888672\n"
     ]
    }
   ],
   "source": [
    "matrices = [\"X0\", \"X1\", \"X2\", \"X3\", \"X4\"]\n",
    "entity_list = [\"E0\", \"E1\", \"E2\", \"E3\", \"E4\", \"E5\"]\n",
    "matrix_entity_mapping = {\"E0\": (X0, X1, X2, X3, X4), \"E1\": (X0), \"E2\":(X1), \"E3\":(X2), \"E4\":(X3), \"E5\":(X4)}\n",
    "emb_dim = 50\n",
    "\n",
    "model = matrix_factorization(matrices, entity_list, matrix_entity_mapping, emb_dim)\n",
    "model.init_autoencoders()\n",
    "model.train_autoencoder()\n",
    "embeddings = model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622e4487",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings, \"all_cnn_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84830f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('all_cnn_embeddings.pkl', 'wb') as handle:\n",
    "#     pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdbe2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
