{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49717fad",
   "metadata": {},
   "source": [
    "Matrix structure:\n",
    "```\n",
    "++++   ++++\n",
    "+X0+   +X1+\n",
    "++++   ++++\n",
    "\n",
    "X0 - books x word emb\n",
    "X1 - books x cnn emb\n",
    "\n",
    "E0 - books\n",
    "E1 - word emb\n",
    "E2 - cnn emb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c6ccfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the input cnn embeddings - to ResNet/GoogleNet/INceptionNet/VGG\n",
    "# try reducing learning error for matrix factorization\n",
    "# try diff regressors\n",
    "# try multi label classification\n",
    "# visualization of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10ab6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60235fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../dataset/'\n",
    "X0_file = dataset_folder + \"sentenceBert_emb_tensor.pt\"\n",
    "X1_file = dataset_folder + \"cnn_encoder_decoder_tensor.pt\" #\"googleNet_cnn_emb_tensor.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b8f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 768])\n",
      "torch.Size([5000, 200])\n"
     ]
    }
   ],
   "source": [
    "X0 = torch.load(X0_file)\n",
    "print(X0.size())\n",
    "X1 = torch.load(X1_file)\n",
    "print(X1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3278b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.enc_linear1 = nn.Linear(input_dim, 128)\n",
    "        self.enc_linear2 = nn.Linear(128, embedding_dim)\n",
    "        self.dec_linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.dec_linear2 = nn.Linear(128, input_dim)\n",
    "        self.emb = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.enc_linear2(x)\n",
    "        self.emb = x # return embedding from encoder\n",
    "        x = torch.relu(x)\n",
    "        x = self.dec_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dec_linear2(x)\n",
    "        return x # use x for training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6b38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrix_factorization():\n",
    "    def __init__(self, matrices, entity_list, matrix_entity_mapping, emb_dim):\n",
    "        self.matrices = matrices\n",
    "        self.entity_list = entity_list\n",
    "        self.matrix_entity_mapping = matrix_entity_mapping # {\"E0\": [\"X0\", \"X1\"], \"E1\": [\"X0\"], \"E2\":[\"X1\"]}\n",
    "        self.emb_dim = emb_dim\n",
    "        self.autoencoders = {} # {\"E0\": E0_autoencoder, \"E1\": E1_ae, ...}\n",
    "        self.reconstructed_matrices = {} # {\"X0\": recon_X0, \"X1\": recon_X1, ...}\n",
    "        self.embeddings = {} # {\"E0\": E0_emb, \"E1\": E1_emb, ...}\n",
    "        self.concatenated_matrices = []\n",
    "        self.optim = None\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.batch_size = 50\n",
    "        self.convergence_threshold = 1e-4\n",
    "        self.learning_rate = 0.001\n",
    "        self.epoch_count = 250\n",
    "        \n",
    "    def init_autoencoders(self):\n",
    "        # initialize autoencoder - one for each entity\n",
    "        for entity, matrices in matrix_entity_mapping.items():\n",
    "            if entity == \"E0\":\n",
    "                C_E0 = torch.cat((matrices[0], matrices[1]), dim = 1)\n",
    "                print(C_E0.size())\n",
    "                E0_aec = Autoencoder(C_E0.size(1), self.emb_dim)\n",
    "            elif entity == \"E1\":\n",
    "                C_E1 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E1.size())\n",
    "                E1_aec = Autoencoder(C_E1.size(1), self.emb_dim)\n",
    "            elif entity == \"E2\":\n",
    "                C_E2 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E2.size())\n",
    "                E2_aec = Autoencoder(C_E2.size(1), self.emb_dim)\n",
    "                \n",
    "        self.concatenated_matrices = {\"E0\": C_E0, \"E1\": C_E1,\"E2\": C_E2}\n",
    "        self.autoencoders = {\"E0\": E0_aec, \"E1\": E1_aec, \"E2\": E2_aec}\n",
    "        self.optim = torch.optim.SGD(list(E0_aec.parameters()) + list(E1_aec.parameters()) + list(E2_aec.parameters()), lr = self.learning_rate)\n",
    "    \n",
    "    def train_autoencoder(self):\n",
    "        # training\n",
    "        prev_losses = []\n",
    "        for epoch in range(0,self.epoch_count):\n",
    "            shuffled_indices = {}\n",
    "            avg_loss = {}\n",
    "            ent_emb = {}\n",
    "            for e in self.autoencoders.keys():\n",
    "                a = np.arange(0 , len(self.concatenated_matrices[e]))\n",
    "                shuffled_indices[e] = torch.LongTensor(a) #torch.randperm(self.concatenated_matrices[e].size(0))\n",
    "                ent_emb[e] = torch.zeros(self.concatenated_matrices[e].size(0), self.emb_dim)\n",
    "            \n",
    "            for e in self.concatenated_matrices.keys():\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "                for count in range(0, self.concatenated_matrices[e].size(0), self.batch_size):\n",
    "                    indices = shuffled_indices[e][count:count+self.batch_size] \n",
    "                    minibatch = self.concatenated_matrices[e][indices]\n",
    "                    output = self.autoencoders[e](minibatch)\n",
    "                    ent_emb[e][indices] = self.autoencoders[e].emb # assign emb of the mini batch to entity\n",
    "#                     print(ent_emb[e][indices[0]])\n",
    "                    loss = self.criterion(minibatch, output)\n",
    "                    num_batches += 1\n",
    "                    total_loss += loss\n",
    "                avg_loss[e] = total_loss/num_batches\n",
    "    \n",
    "#             print(ent_emb['E0'][0])\n",
    "            aec_loss = 0\n",
    "    \n",
    "            for v in avg_loss.values():\n",
    "                aec_loss += v\n",
    "#             print(f\"Aec {aec_loss}\")\n",
    "            self.reconstructed_matrices['X0'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E1'], 0, 1))\n",
    "            self.reconstructed_matrices['X1'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E2'], 0, 1))\n",
    "            recon_loss = self.criterion(self.reconstructed_matrices['X0'], self.matrix_entity_mapping[\"E1\"]) + \\\n",
    "                        self.criterion(self.reconstructed_matrices['X1'], self.matrix_entity_mapping[\"E2\"])\n",
    "#             print(f\"recon loss {recon_loss}\")\n",
    "            aec_loss += recon_loss\n",
    "#             print(f\"Total {aec_loss}\")\n",
    "            self.optim.zero_grad()\n",
    "            aec_loss.requires_grad_(True)\n",
    "            aec_loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Average loss for epoch {epoch} = {aec_loss}\")\n",
    "            if  (epoch > 100) and (len(prev_losses) > 0) and (prev_losses[-1] - aec_loss < self.convergence_threshold):\n",
    "                print('Convergence!')\n",
    "                break\n",
    "            prev_losses.append(aec_loss)\n",
    "        \n",
    "    def get_embeddings(self):\n",
    "        for e in self.matrix_entity_mapping.keys():\n",
    "            out = self.autoencoders[e](self.concatenated_matrices[e])\n",
    "            self.embeddings[e] = self.autoencoders[e].emb\n",
    "        return self.embeddings\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94449844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 968])\n",
      "torch.Size([768, 5000])\n",
      "torch.Size([200, 5000])\n",
      "Average loss for epoch 0 = 264.88128662109375\n",
      "Average loss for epoch 10 = nan\n",
      "Average loss for epoch 20 = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-05c689e2a639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix_factorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix_entity_mapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0memb_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_autoencoders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_autoencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0membeddings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-d318cc9bcc7c>\u001b[0m in \u001b[0;36mtrain_autoencoder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m                     \u001b[0ment_emb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautoencoders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memb\u001b[0m \u001b[1;31m# assign emb of the mini batch to entity\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;31m#                     print(ent_emb[e][indices[0]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m                     \u001b[0mnum_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[0mtotal_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91984\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91984\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\91984\\anaconda3\\envs\\gpuenv\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2183\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2184\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2185\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2187\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'mean'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "matrices = [\"X0\", \"X1\"]\n",
    "entity_list = [\"E0\", \"E1\", \"E2\"]\n",
    "matrix_entity_mapping = {\"E0\": (X0, X1), \"E1\": (X0), \"E2\":(X1)}\n",
    "emb_dim = 50\n",
    "\n",
    "model = matrix_factorization(matrices, entity_list, matrix_entity_mapping, emb_dim)\n",
    "model.init_autoencoders()\n",
    "model.train_autoencoder()\n",
    "embeddings = model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84830f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('embeddings.pkl', 'wb') as handle:\n",
    "#     pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings, \"sentenceBert_googleNet_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = torch.load(\"../dataset/train_book_id.pkl\")\n",
    "test_ids = torch.load(\"../dataset/test_book_id.pkl\")\n",
    "print(train_ids.size())\n",
    "print(test_ids.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/books_with_genres.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b47205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.book_id.isin(train_ids.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = embeddings[\"E0\"][0:4000].detach().numpy()\n",
    "X_test = embeddings[\"E0\"][4000:].detach().numpy()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23396d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[\"average_rating\"][0:4000]\n",
    "y_test = df[\"average_rating\"][4000:]\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856074ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict(X_test)\n",
    "error = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n",
    "preds = svr.predict(X_test)\n",
    "error = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba36b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fbae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(X_train), columns = [\"comp1\", \"comp2\"])\n",
    "df_pca[\"y_train\"] = y_train \n",
    "print(df_pca.shape)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca[\"y_cat\"] = df_pca[\"y_train\"].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab25f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d250c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d927bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"comp1\", y=\"comp2\", data=df_pca, hue=\"y_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdbe2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
