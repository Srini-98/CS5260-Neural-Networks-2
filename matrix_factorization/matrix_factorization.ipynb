{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49717fad",
   "metadata": {},
   "source": [
    "Matrix structure:\n",
    "```\n",
    "++++   ++++\n",
    "+X0+   +X1+\n",
    "++++   ++++\n",
    "\n",
    "X0 - books x word emb\n",
    "X1 - books x cnn emb\n",
    "\n",
    "E0 - books\n",
    "E1 - word emb\n",
    "E2 - cnn emb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ab6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60235fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../dataset/'\n",
    "X0_file = dataset_folder + \"word2vec_emb_tensor.pkl\"\n",
    "X1_file = dataset_folder + \"cnn_emb_tensor.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b8f300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 100])\n",
      "torch.Size([5000, 1000])\n"
     ]
    }
   ],
   "source": [
    "X0 = torch.load(X0_file)\n",
    "print(X0.size())\n",
    "X1 = torch.load(X1_file)\n",
    "print(X1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3278b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.enc_linear1 = nn.Linear(input_dim, 128)\n",
    "        self.enc_linear2 = nn.Linear(128, embedding_dim)\n",
    "        self.dec_linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.dec_linear2 = nn.Linear(128, input_dim)\n",
    "        self.emb = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.enc_linear2(x)\n",
    "        self.emb = x # return embedding from encoder\n",
    "        x = torch.relu(x)\n",
    "        x = self.dec_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dec_linear2(x)\n",
    "        return x # use x for training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6b38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrix_factorization():\n",
    "    def __init__(self, matrices, entity_list, matrix_entity_mapping, emb_dim):\n",
    "        self.matrices = matrices\n",
    "        self.entity_list = entity_list\n",
    "        self.matrix_entity_mapping = matrix_entity_mapping # {\"E0\": [\"X0\", \"X1\"], \"E1\": [\"X0\"], \"E2\":[\"X1\"]}\n",
    "        self.emb_dim = emb_dim\n",
    "        self.autoencoders = {} # {\"E0\": E0_autoencoder, \"E1\": E1_ae, ...}\n",
    "        self.reconstructed_matrices = {} # {\"X0\": recon_X0, \"X1\": recon_X1, ...}\n",
    "        self.embeddings = {} # {\"E0\": E0_emb, \"E1\": E1_emb, ...}\n",
    "        self.concatenated_matrices = []\n",
    "        self.optim = None\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.batch_size = 50\n",
    "        self.convergence_threshold = 1e-4\n",
    "        self.learning_rate = 0.1\n",
    "        self.epoch_count = 1000\n",
    "        \n",
    "    def init_autoencoders(self):\n",
    "        # initialize autoencoder - one for each entity\n",
    "        for entity, matrices in matrix_entity_mapping.items():\n",
    "            if entity == \"E0\":\n",
    "                C_E0 = torch.cat((matrices[0], matrices[1]), dim = 1)\n",
    "                print(C_E0.size())\n",
    "                E0_aec = Autoencoder(C_E0.size(1), self.emb_dim)\n",
    "            elif entity == \"E1\":\n",
    "                C_E1 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E1.size())\n",
    "                E1_aec = Autoencoder(C_E1.size(1), self.emb_dim)\n",
    "            elif entity == \"E2\":\n",
    "                C_E2 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E2.size())\n",
    "                E2_aec = Autoencoder(C_E2.size(1), self.emb_dim)\n",
    "                \n",
    "        self.concatenated_matrices = {\"E0\": C_E0, \"E1\": C_E1,\"E2\": C_E2}\n",
    "        self.autoencoders = {\"E0\": E0_aec, \"E1\": E1_aec, \"E2\": E2_aec}\n",
    "        self.optim = torch.optim.SGD(list(E0_aec.parameters()) + list(E1_aec.parameters()) + list(E2_aec.parameters()), lr = self.learning_rate)\n",
    "    \n",
    "    def train_autoencoder(self):\n",
    "        # training\n",
    "        prev_losses = []\n",
    "        for epoch in range(0,self.epoch_count):\n",
    "            shuffled_indices = {}\n",
    "            avg_loss = {}\n",
    "            for e in self.autoencoders.keys():\n",
    "                shuffled_indices[e] = torch.randperm(self.concatenated_matrices[e].size(0))\n",
    "            \n",
    "            for e in self.concatenated_matrices.keys():\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "                for count in range(0, self.concatenated_matrices[e].size(0), self.batch_size):\n",
    "                    indices = shuffled_indices[e][count:count+self.batch_size]\n",
    "                    minibatch = self.concatenated_matrices[e][indices]\n",
    "                    output = self.autoencoders[e](minibatch)\n",
    "                    loss = self.criterion(minibatch, output)\n",
    "                    num_batches += 1\n",
    "                    total_loss += loss\n",
    "                avg_loss[e] = total_loss/num_batches\n",
    "    \n",
    "                \n",
    "            aec_loss = 0\n",
    "    \n",
    "            for v in avg_loss.values():\n",
    "                aec_loss += v\n",
    "    \n",
    "            self.optim.zero_grad()\n",
    "            aec_loss.requires_grad_(True)\n",
    "            aec_loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Average loss for epoch {epoch} = {aec_loss}\")\n",
    "            if  (epoch > 100) and (len(prev_losses) > 0) and (prev_losses[-1] - aec_loss < self.convergence_threshold):\n",
    "                print('Convergence!')\n",
    "                break\n",
    "            prev_losses.append(aec_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94449844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 1100])\n",
      "torch.Size([100, 5000])\n",
      "torch.Size([1000, 5000])\n",
      "Average loss for epoch 0 = 3.781221866607666\n",
      "Average loss for epoch 10 = 3.7677924633026123\n",
      "Average loss for epoch 20 = 3.75838565826416\n",
      "Average loss for epoch 30 = 3.747037649154663\n",
      "Average loss for epoch 40 = 3.729417085647583\n",
      "Average loss for epoch 50 = 3.6968374252319336\n",
      "Average loss for epoch 60 = 3.633277416229248\n",
      "Average loss for epoch 70 = 3.5241165161132812\n",
      "Average loss for epoch 80 = 3.3786439895629883\n",
      "Average loss for epoch 90 = 3.2400107383728027\n",
      "Average loss for epoch 100 = 3.146082878112793\n",
      "Average loss for epoch 110 = 3.0962650775909424\n",
      "Average loss for epoch 120 = 3.0727665424346924\n",
      "Average loss for epoch 130 = 3.061732769012451\n",
      "Average loss for epoch 140 = 3.0559823513031006\n",
      "Average loss for epoch 150 = 3.0523102283477783\n",
      "Average loss for epoch 160 = 3.049363613128662\n",
      "Average loss for epoch 170 = 3.0465664863586426\n",
      "Average loss for epoch 180 = 3.0436673164367676\n",
      "Average loss for epoch 190 = 3.040558338165283\n",
      "Average loss for epoch 200 = 3.0371737480163574\n",
      "Average loss for epoch 210 = 3.0334689617156982\n",
      "Average loss for epoch 220 = 3.02940034866333\n",
      "Average loss for epoch 230 = 3.0249288082122803\n",
      "Average loss for epoch 240 = 3.020007610321045\n",
      "Average loss for epoch 250 = 3.0145909786224365\n",
      "Average loss for epoch 260 = 3.008629560470581\n",
      "Average loss for epoch 270 = 3.0020668506622314\n",
      "Average loss for epoch 280 = 2.994852066040039\n",
      "Average loss for epoch 290 = 2.986934185028076\n",
      "Average loss for epoch 300 = 2.978255271911621\n"
     ]
    }
   ],
   "source": [
    "matrices = [\"X0\", \"X1\"]\n",
    "entity_list = [\"E0\", \"E1\", \"E2\"]\n",
    "matrix_entity_mapping = {\"E0\": (X0, X1), \"E1\": (X0), \"E2\":(X1)}\n",
    "emb_dim = 50\n",
    "\n",
    "model = matrix_factorization(matrices, entity_list, matrix_entity_mapping, emb_dim)\n",
    "model.init_autoencoders()\n",
    "model.train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84830f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3468b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2710f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e053a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
