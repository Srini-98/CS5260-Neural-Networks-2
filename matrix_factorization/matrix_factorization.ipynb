{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49717fad",
   "metadata": {},
   "source": [
    "Matrix structure:\n",
    "```\n",
    "++++   ++++\n",
    "+X0+   +X1+\n",
    "++++   ++++\n",
    "\n",
    "X0 - books x word emb\n",
    "X1 - books x cnn emb\n",
    "\n",
    "E0 - books\n",
    "E1 - word emb\n",
    "E2 - cnn emb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6ccfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the input cnn embeddings - to ResNet/GoogleNet/INceptionNet/VGG\n",
    "# try reducing learning error for matrix factorization\n",
    "# try diff regressors\n",
    "# try multi label classification\n",
    "# visualization of embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ab6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60235fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../dataset/'\n",
    "X0_file = dataset_folder + \"sentenceBert_emb_tensor.pt\"\n",
    "X1_file = dataset_folder + \"cnn_encoder_decoder_tensor.pt\" #\"googleNet_cnn_emb_tensor.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b8f300",
   "metadata": {},
   "outputs": [],
   "source": [
    "X0 = torch.load(X0_file)\n",
    "print(X0.size())\n",
    "X1 = torch.load(X1_file)\n",
    "print(X1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3278b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.enc_linear1 = nn.Linear(input_dim, 128)\n",
    "        self.enc_linear2 = nn.Linear(128, embedding_dim)\n",
    "        self.dec_linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.dec_linear2 = nn.Linear(128, input_dim)\n",
    "        self.emb = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.enc_linear2(x)\n",
    "        self.emb = x # return embedding from encoder\n",
    "        x = torch.relu(x)\n",
    "        x = self.dec_linear1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dec_linear2(x)\n",
    "        return x # use x for training\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6b38f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class matrix_factorization():\n",
    "    def __init__(self, matrices, entity_list, matrix_entity_mapping, emb_dim):\n",
    "        self.matrices = matrices\n",
    "        self.entity_list = entity_list\n",
    "        self.matrix_entity_mapping = matrix_entity_mapping # {\"E0\": [\"X0\", \"X1\"], \"E1\": [\"X0\"], \"E2\":[\"X1\"]}\n",
    "        self.emb_dim = emb_dim\n",
    "        self.autoencoders = {} # {\"E0\": E0_autoencoder, \"E1\": E1_ae, ...}\n",
    "        self.reconstructed_matrices = {} # {\"X0\": recon_X0, \"X1\": recon_X1, ...}\n",
    "        self.embeddings = {} # {\"E0\": E0_emb, \"E1\": E1_emb, ...}\n",
    "        self.concatenated_matrices = []\n",
    "        self.optim = None\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.batch_size = 50\n",
    "        self.convergence_threshold = 1e-4\n",
    "        self.learning_rate = 0.000001\n",
    "        self.epoch_count = 250\n",
    "        \n",
    "    def init_autoencoders(self):\n",
    "        # initialize autoencoder - one for each entity\n",
    "        for entity, matrices in matrix_entity_mapping.items():\n",
    "            if entity == \"E0\":\n",
    "                C_E0 = torch.cat((matrices[0], matrices[1]), dim = 1)\n",
    "                print(C_E0.size())\n",
    "                E0_aec = Autoencoder(C_E0.size(1), self.emb_dim)\n",
    "            elif entity == \"E1\":\n",
    "                C_E1 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E1.size())\n",
    "                E1_aec = Autoencoder(C_E1.size(1), self.emb_dim)\n",
    "            elif entity == \"E2\":\n",
    "                C_E2 = torch.transpose(matrices, 0, 1)\n",
    "                print(C_E2.size())\n",
    "                E2_aec = Autoencoder(C_E2.size(1), self.emb_dim)\n",
    "                \n",
    "        self.concatenated_matrices = {\"E0\": C_E0, \"E1\": C_E1,\"E2\": C_E2}\n",
    "        self.autoencoders = {\"E0\": E0_aec, \"E1\": E1_aec, \"E2\": E2_aec}\n",
    "        self.optim = torch.optim.SGD(list(E0_aec.parameters()) + list(E1_aec.parameters()) + list(E2_aec.parameters()), lr = self.learning_rate)\n",
    "    \n",
    "    def train_autoencoder(self):\n",
    "        # training\n",
    "        prev_losses = []\n",
    "        for epoch in range(0,self.epoch_count):\n",
    "            shuffled_indices = {}\n",
    "            avg_loss = {}\n",
    "            ent_emb = {}\n",
    "            for e in self.autoencoders.keys():\n",
    "                a = np.arange(0 , len(self.concatenated_matrices[e]))\n",
    "                shuffled_indices[e] = torch.LongTensor(a) #torch.randperm(self.concatenated_matrices[e].size(0))\n",
    "                ent_emb[e] = torch.zeros(self.concatenated_matrices[e].size(0), self.emb_dim)\n",
    "            \n",
    "            for e in self.concatenated_matrices.keys():\n",
    "                total_loss = 0\n",
    "                num_batches = 0\n",
    "                for count in range(0, self.concatenated_matrices[e].size(0), self.batch_size):\n",
    "                    indices = shuffled_indices[e][count:count+self.batch_size] \n",
    "                    minibatch = self.concatenated_matrices[e][indices]\n",
    "                    output = self.autoencoders[e](minibatch)\n",
    "                    ent_emb[e][indices] = self.autoencoders[e].emb # assign emb of the mini batch to entity\n",
    "#                     print(ent_emb[e][indices[0]])\n",
    "                    loss = self.criterion(minibatch, output)\n",
    "                    num_batches += 1\n",
    "                    total_loss += loss\n",
    "                avg_loss[e] = total_loss/num_batches\n",
    "    \n",
    "#             print(ent_emb['E0'][0])\n",
    "            aec_loss = 0\n",
    "    \n",
    "            for v in avg_loss.values():\n",
    "                aec_loss += v\n",
    "#             print(f\"Aec {aec_loss}\")\n",
    "            self.reconstructed_matrices['X0'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E1'], 0, 1))\n",
    "            self.reconstructed_matrices['X1'] = torch.matmul(ent_emb['E0'], torch.transpose(ent_emb['E2'], 0, 1))\n",
    "            recon_loss = self.criterion(self.reconstructed_matrices['X0'], self.matrix_entity_mapping[\"E1\"]) + \\\n",
    "                        self.criterion(self.reconstructed_matrices['X1'], self.matrix_entity_mapping[\"E2\"])\n",
    "#             print(f\"recon loss {recon_loss}\")\n",
    "            aec_loss += recon_loss\n",
    "#             print(f\"Total {aec_loss}\")\n",
    "            self.optim.zero_grad()\n",
    "            aec_loss.requires_grad_(True)\n",
    "            aec_loss.backward()\n",
    "            self.optim.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Average loss for epoch {epoch} = {aec_loss}\")\n",
    "            if  (epoch > 100) and (len(prev_losses) > 0) and (prev_losses[-1] - aec_loss < self.convergence_threshold):\n",
    "                print('Convergence!')\n",
    "                break\n",
    "            prev_losses.append(aec_loss)\n",
    "        \n",
    "    def get_embeddings(self):\n",
    "        for e in self.matrix_entity_mapping.keys():\n",
    "            out = self.autoencoders[e](self.concatenated_matrices[e])\n",
    "            self.embeddings[e] = self.autoencoders[e].emb\n",
    "        return self.embeddings\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94449844",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = [\"X0\", \"X1\"]\n",
    "entity_list = [\"E0\", \"E1\", \"E2\"]\n",
    "matrix_entity_mapping = {\"E0\": (X0, X1), \"E1\": (X0), \"E2\":(X1)}\n",
    "emb_dim = 50\n",
    "\n",
    "model = matrix_factorization(matrices, entity_list, matrix_entity_mapping, emb_dim)\n",
    "model.init_autoencoders()\n",
    "model.train_autoencoder()\n",
    "embeddings = model.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84830f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('embeddings.pkl', 'wb') as handle:\n",
    "#     pickle.dump(embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4636eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(embeddings, \"sentenceBert_cnn_encoder_decoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = torch.load(\"../dataset/train_book_id.pkl\")\n",
    "test_ids = torch.load(\"../dataset/test_book_id.pkl\")\n",
    "print(train_ids.size())\n",
    "print(test_ids.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6e053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../dataset/books_with_genres.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b47205",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.book_id.isin(train_ids.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = embeddings[\"E0\"][0:4000].detach().numpy()\n",
    "X_test = embeddings[\"E0\"][4000:].detach().numpy()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23396d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[\"average_rating\"][0:4000]\n",
    "y_test = df[\"average_rating\"][4000:]\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856074ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "preds = lr.predict(X_test)\n",
    "error = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR()\n",
    "svr.fit(X_train, y_train)\n",
    "preds = svr.predict(X_test)\n",
    "error = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba36b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(error)\n",
    "print(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45fbae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "df_pca = pd.DataFrame(pca.fit_transform(X_train), columns = [\"comp1\", \"comp2\"])\n",
    "df_pca[\"y_train\"] = y_train \n",
    "print(df_pca.shape)\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914f4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca[\"y_cat\"] = df_pca[\"y_train\"].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab25f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d250c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d927bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"comp1\", y=\"comp2\", data=df_pca, hue=\"y_cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdbe2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
